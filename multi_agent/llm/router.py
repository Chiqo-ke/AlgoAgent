"""
Request Router for centralized LLM API calls.

Provides:
- Intelligent key selection through KeyManager
- Conversation state management
- Automatic retry with backoff on 429 errors
- Key cooldown management
- Metrics and logging
"""
import time
import random
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

from keys.manager import KeyManager, get_key_manager, KeySelectionError
from conversation.store import ConversationStore, get_conversation_store
from llm.token_utils import estimate_tokens, estimate_completion_tokens
from llm.providers import get_provider_client, ProviderError, RateLimitError, SafetyBlockError

logger = logging.getLogger(__name__)


class RouterError(Exception):
    """Base error for router operations."""
    pass


class AllKeysExhaustedError(RouterError):
    """Raised when all keys are exhausted or rate limited."""
    pass


class RequestRouter:
    """
    Centralized LLM request router.
    
    Handles:
    - Key selection and rotation
    - Conversation history management
    - Token estimation and TPM enforcement
    - Retry logic with exponential backoff
    - Error handling and cooldown management
    """
    
    def __init__(
        self,
        key_manager: Optional[KeyManager] = None,
        conv_store: Optional[ConversationStore] = None,
        max_retries: Optional[int] = None,
        base_backoff_ms: Optional[int] = None
    ):
        """
        Initialize request router.
        
        Args:
            key_manager: KeyManager instance
            conv_store: ConversationStore instance
            max_retries: Maximum retry attempts (default: 3, or from env LLM_MAX_RETRIES)
            base_backoff_ms: Base backoff duration in milliseconds (default: 500)
        """
        self.key_manager = key_manager or get_key_manager()
        self.conv_store = conv_store or get_conversation_store()
        
        # Get max retries from env or use provided/default
        import os
        self.max_retries = max_retries if max_retries is not None else int(os.getenv('LLM_MAX_RETRIES', '3'))
        self.base_backoff_ms = base_backoff_ms if base_backoff_ms is not None else int(os.getenv('LLM_BASE_BACKOFF_MS', '500'))
        
        logger.info(f"Request router initialized (max_retries={self.max_retries}, base_backoff_ms={self.base_backoff_ms})")
    
    def send_chat(
        self,
        conv_id: str,
        prompt: str,
        user_id: Optional[str] = None,
        model_preference: Optional[str] = None,
        expected_completion_tokens: int = 512,
        max_output_tokens: int = 2048,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        workload: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a chat message and get response.
        
        Args:
            conv_id: Conversation identifier
            prompt: User prompt
            user_id: User identifier (for tracking)
            model_preference: Preferred model (e.g., "gemini-2.5-pro")
            expected_completion_tokens: Expected response length
            max_output_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            system_prompt: System prompt (if starting new conversation)
            metadata: Additional metadata to store
            workload: Workload type - "light" (flash), "medium" (pro), "heavy" (pro-preview)
            
        Returns:
            {
                'success': bool,
                'content': str,  # Assistant response
                'model': str,
                'key_id': str,
                'tokens': {
                    'input': int,
                    'output': int,
                    'total': int
                },
                'conversation_id': str,
                'error': str  # If failed
            }
        """
        try:
            # Ensure conversation exists
            if not self.conv_store.get_metadata(conv_id):
                meta = {'user_id': user_id} if user_id else {}
                if metadata:
                    meta.update(metadata)
                self.conv_store.create_conversation(conv_id, meta)
                
                # Add system prompt if provided
                if system_prompt:
                    self.conv_store.append_message(
                        conv_id, 'system', system_prompt
                    )
            
            # Build message history
            history = self.conv_store.get_history(conv_id)
            
            # Add user message
            self.conv_store.append_message(conv_id, 'user', prompt)
            messages = history + [{'role': 'user', 'content': prompt}]
            
            # Estimate tokens needed
            tokens_needed = estimate_tokens(messages, expected_completion_tokens)
            
            logger.info(
                f"Sending chat for conv_id={conv_id}, "
                f"estimated_tokens={tokens_needed}, "
                f"model_preference={model_preference}, "
                f"workload={workload}"
            )
            
            # Try to send with retries
            excluded_keys = []
            for attempt in range(self.max_retries + 1):
                try:
                    # Select key
                    key_meta = self.key_manager.select_key(
                        model_preference=model_preference,
                        tokens_needed=tokens_needed,
                        exclude_keys=excluded_keys,
                        workload=workload
                    )
                    
                    if not key_meta:
                        raise AllKeysExhaustedError(
                            "No keys with available capacity"
                        )
                    
                    # Make API call
                    response = self._call_provider(
                        messages=messages,
                        key_meta=key_meta,
                        max_output_tokens=max_output_tokens,
                        temperature=temperature
                    )
                    
                    # Success - save assistant response
                    self.conv_store.append_message(
                        conv_id,
                        'assistant',
                        response['content'],
                        tokens=response.get('tokens', {}).get('output'),
                        metadata={
                            'model': response['model'],
                            'key_id': key_meta['key_id']
                        }
                    )
                    
                    response['conversation_id'] = conv_id
                    response['success'] = True
                    
                    logger.info(
                        f"Chat successful: conv_id={conv_id}, "
                        f"model={response['model']}, "
                        f"tokens={response.get('tokens', {})}"
                    )
                    
                    return response
                    
                except SafetyBlockError as e:
                    # Handle safety blocks - DON'T mark key unhealthy (content issue, not API issue)
                    logger.warning(
                        f"Safety block for key {key_meta['key_id']}: {e}"
                    )
                    
                    # Strategy 1: Escalate model tier (Pro models less sensitive)
                    if workload == "light" and attempt < self.max_retries:
                        logger.info("Escalating from light to medium workload due to safety block")
                        workload = "medium"
                        continue
                    elif workload == "medium" and attempt < self.max_retries:
                        logger.info("Escalating from medium to heavy workload due to safety block")
                        workload = "heavy"
                        continue
                    
                    # Strategy 2: Last attempt - sanitize prompt
                    elif attempt == self.max_retries:
                        logger.warning("Last attempt: sanitizing prompt to bypass safety filter")
                        messages = self._sanitize_prompt(messages)
                        # Don't exclude key - retry with sanitized prompt
                        continue
                    else:
                        # Can't escalate further
                        raise RouterError(
                            f"Content blocked by safety filter after all escalation attempts. "
                            f"Safety ratings: {e.safety_ratings}"
                        )
                    
                except RateLimitError as e:
                    # Handle rate limit
                    logger.warning(
                        f"Rate limit for key {key_meta['key_id']}: {e}"
                    )
                    
                    # Set cooldown
                    cooldown_seconds = e.retry_after or 60
                    self.key_manager.mark_key_unhealthy(
                        key_meta['key_id'],
                        cooldown_seconds=cooldown_seconds,
                        reason=f"Rate limit (429): {str(e)}"
                    )
                    
                    # Exclude this key from next attempt
                    excluded_keys.append(key_meta['key_id'])
                    
                    # Backoff before retry
                    if attempt < self.max_retries:
                        backoff_ms = self._calculate_backoff(attempt)
                        logger.info(f"Retrying after {backoff_ms}ms backoff")
                        time.sleep(backoff_ms / 1000)
                    else:
                        raise AllKeysExhaustedError(
                            f"All retry attempts exhausted: {str(e)}"
                        )
                
                except ProviderError as e:
                    # Check if this is a retryable error
                    error_str = str(e).lower()
                    is_retryable = any(keyword in error_str for keyword in [
                        '504', 'deadline exceeded', 'timeout', 
                        '503', 'service unavailable', 'temporarily unavailable',
                        '502', 'bad gateway', 'connection'
                    ])
                    
                    if is_retryable and attempt < self.max_retries:
                        logger.warning(
                            f"Retryable provider error on attempt {attempt + 1}/{self.max_retries + 1}: {e}"
                        )
                        
                        # Set short cooldown for this key
                        if key_meta:
                            self.key_manager.mark_key_unhealthy(
                                key_meta['key_id'],
                                cooldown_seconds=30,
                                reason=f"Retryable error: {str(e)}"
                            )
                            excluded_keys.append(key_meta['key_id'])
                        
                        # Exponential backoff before retry
                        backoff_ms = self._calculate_backoff(attempt)
                        logger.info(f"Retrying after {backoff_ms}ms backoff (different key)")
                        time.sleep(backoff_ms / 1000)
                        continue  # Retry with different key
                    else:
                        # Non-retryable error or max retries exceeded
                        logger.error(f"Non-retryable provider error: {e}")
                        
                        if key_meta:
                            self.key_manager.mark_key_unhealthy(
                                key_meta['key_id'],
                                cooldown_seconds=30,
                                reason=f"Provider error: {str(e)}"
                            )
                        
                        raise RouterError(f"Provider error: {str(e)}")
            
            # Should not reach here
            raise AllKeysExhaustedError("Max retries exceeded")
            
        except AllKeysExhaustedError as e:
            logger.error(f"All keys exhausted: {e}")
            return {
                'success': False,
                'error': str(e),
                'error_type': 'rate_limited',
                'conversation_id': conv_id
            }
            
        except Exception as e:
            logger.exception(f"Unexpected error in send_chat: {e}")
            return {
                'success': False,
                'error': str(e),
                'error_type': 'internal_error',
                'conversation_id': conv_id
            }
    
    def _call_provider(
        self,
        messages: List[Dict[str, str]],
        key_meta: Dict[str, Any],
        max_output_tokens: int,
        temperature: float
    ) -> Dict[str, Any]:
        """
        Call provider API.
        
        Returns:
            {
                'content': str,
                'model': str,
                'key_id': str,
                'tokens': {
                    'input': int,
                    'output': int,
                    'total': int
                }
            }
        """
        provider = key_meta['provider']
        api_key = key_meta['secret']
        model = key_meta['model']
        
        # Get provider client
        client = get_provider_client(provider)
        
        # Make request
        start_time = time.time()
        
        response = client.chat_completion(
            api_key=api_key,
            model=model,
            messages=messages,
            max_tokens=max_output_tokens,
            temperature=temperature
        )
        
        duration_ms = int((time.time() - start_time) * 1000)
        
        logger.debug(
            f"Provider call completed: model={model}, "
            f"duration={duration_ms}ms, "
            f"tokens={response.get('tokens')}"
        )
        
        response['key_id'] = key_meta['key_id']
        response['duration_ms'] = duration_ms
        
        return response
    
    def _calculate_backoff(self, attempt: int) -> int:
        """
        Calculate exponential backoff with jitter.
        
        Args:
            attempt: Retry attempt number (0-indexed)
            
        Returns:
            Backoff duration in milliseconds
        """
        # Exponential: base * 2^attempt
        backoff = self.base_backoff_ms * (2 ** attempt)
        
        # Add jitter (Â±25%)
        jitter = random.uniform(-0.25, 0.25) * backoff
        backoff = int(backoff + jitter)
        
        # Cap at 30 seconds
        return min(backoff, 30000)
    
    def send_one_shot(
        self,
        prompt: str,
        model_preference: Optional[str] = None,
        system_prompt: Optional[str] = None,
        expected_completion_tokens: int = 512,
        max_output_tokens: int = 2048,
        temperature: float = 0.7,
        workload: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a one-shot request (no conversation history).
        
        Useful for stateless operations like code generation.
        
        Args:
            prompt: User prompt
            model_preference: Preferred model
            system_prompt: System prompt
            expected_completion_tokens: Expected response length
            max_output_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            workload: Workload type - "light" (flash), "medium" (pro), "heavy" (pro-preview)
            
        Returns:
            Same as send_chat()
        """
        # Create temporary conversation ID
        conv_id = f"oneshot_{int(time.time() * 1000)}"
        
        return self.send_chat(
            conv_id=conv_id,
            prompt=prompt,
            model_preference=model_preference,
            system_prompt=system_prompt,
            expected_completion_tokens=expected_completion_tokens,
            max_output_tokens=max_output_tokens,
            temperature=temperature,
            workload=workload
        )
    
    def get_conversation(self, conv_id: str) -> Dict[str, Any]:
        """
        Get conversation details.
        
        Returns:
            {
                'conv_id': str,
                'messages': List[Dict],
                'metadata': Dict
            }
        """
        return {
            'conv_id': conv_id,
            'messages': self.conv_store.get_history(conv_id, include_metadata=True),
            'metadata': self.conv_store.get_metadata(conv_id)
        }
    
    def truncate_conversation(self, conv_id: str, keep_last_n: int = 20):
        """Truncate conversation to manage context length."""
        self.conv_store.truncate_history(conv_id, keep_last_n)
    
    def _sanitize_prompt(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Remove potentially triggering content from prompt.
        
        This is a last-resort fallback when safety filters trigger.
        Removes code blocks and aggressive language while preserving instructions.
        
        Args:
            messages: Original message list
            
        Returns:
            Sanitized message list
        """
        import re
        
        sanitized = []
        for msg in messages:
            content = msg["content"]
            
            # Remove code blocks that might trigger safety filters
            content = re.sub(r'```[\s\S]*?```', '[CODE_BLOCK_REMOVED]', content)
            
            # Remove inline code
            content = re.sub(r'`[^`]+`', '[CODE]', content)
            
            # Soften aggressive trading language
            replacements = {
                r'\bkill\b': 'close',
                r'\bexploit\b': 'use',
                r'\battack\b': 'strategy',
                r'\baggressive\b': 'active',
                r'\bhft\b': 'high-frequency trading',
                r'\bmanipulat': 'optimiz',
            }
            
            for pattern, replacement in replacements.items():
                content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
            
            sanitized.append({"role": msg["role"], "content": content})
        
        logger.debug(f"Sanitized {len(messages)} messages for safety filter bypass")
        return sanitized
    
    def health_check(self) -> Dict[str, Any]:
        """
        Perform health check on router and dependencies.
        
        Returns:
            {
                'healthy': bool,
                'key_manager': Dict,
                'conversation_store': bool
            }
        """
        km_health = self.key_manager.health_check()
        conv_health = self.conv_store.health_check()
        
        return {
            'healthy': km_health['healthy'] and conv_health,
            'key_manager': km_health,
            'conversation_store': conv_health,
            'timestamp': datetime.utcnow().isoformat()
        }


# Singleton instance
_request_router: Optional[RequestRouter] = None


def get_request_router(
    key_manager: Optional[KeyManager] = None,
    conv_store: Optional[ConversationStore] = None
) -> RequestRouter:
    """Get or create singleton request router."""
    global _request_router
    if _request_router is None:
        _request_router = RequestRouter(key_manager, conv_store)
    return _request_router


def reset_request_router():
    """Reset singleton (for testing)."""
    global _request_router
    _request_router = None
